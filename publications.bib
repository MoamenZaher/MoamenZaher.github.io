@article{zaher2024unlocking,
  title={Unlocking the potential of RNN and CNN models for accurate rehabilitation exercise classification on multi-datasets},
  author={Zaher, Moamen and Ghoneim, Amr S and Abdelhamid, Laila and Atia, Ayman},
  journal={Multimedia Tools and Applications},
  pages={1--41},
  year={2024},
  month={4}
  publisher={Springer},
  issn={1380-7501},
  eissn={1573-7721},
  doi={10.1007/s11042-024-19092-0},
  url={https://doi.org/10.1007/s11042-024-19092-0}
}

@inproceedings{amgad2023robust,
  title={A Robust Ensemble Deep Learning Approach for Breast Cancer Diagnosis},
  author={Amgad, Nadeen and Ahmed, Mariam and Haitham, Haidy and Zaher, Moamen and Mohammed, Ammar},
  booktitle={2023 Intelligent Methods, Systems, and Applications (IMSA)},
  pages={452--457},
  year={2023},
  month={7},
  organization={IEEE},
  doi={10.1109/IMSA58542.2023.10217501}
}

@inproceedings{walid2024scoring,
  title={A Scoring Approach for Improving Presentation Impact: Addressing Voice Stuttering, AR Glasses-Based Emotion Recognition, and Profiled Movement Assessment},
  author={Walid, Mazen and Ameen, Mostafa and Zaher, Moamen and Atia, Ayman},
  booktitle={2024 6th International Conference on Computing and Informatics (ICCI)},
  pages={284--290},
  year={2024},
  month={3},
  organization={IEEE}
}

@inproceedings{zaher2023framework,
  title={A Framework for Assessing Physical Rehabilitation Exercises},
  author={Zaher, Moamen and Samir, Ahmed and Ghoneim, Amr and Abdelhamid, Laila and Atia, Ayman},
  booktitle={2023 Intelligent Methods, Systems, and Applications (IMSA)},
  pages={526--532},
  year={2023},
  month={7}
  organization={IEEE},
  doi={10.1109/IMSA58542.2023.10217392}
}
@article{zaher2023comparative,
  title={Comparative Study Between Machine learning algorithms and feature ranking techniques on UI-PRMD dataset},
  author={Zaher, Moamen and Ghoneem, Amr and Abdelhamid, Laila and Ezzat, Ayman},
  year={2023},
  doi={https://doi.org/10.21203/rs.3.rs-2821786/v1}
}

@article{zaher2024artificial,
  title={Artificial Intelligence Techniques in Enhancing Home-Based Rehabilitation: A Survey},
  author={Zaher, Moamen and Atia, Ayman and Ghoneim, Amr and Abdelhamid, Laila},
  journal={FCI-H Informatics Bulletin},
  volume = {6},
  month={7}
  number = {2},
  pages = {16-30},
  year={2024},
  issn = {2537-0901}, 
  eissn = {2535-1397}, 
  doi = {10.21608/fcihib.2024.280269.1113},
  keywords = {Rehabilitation,Skeleton-based,Machine Learning,fusion},
  url = {https://fcihib.journals.ekb.eg/article_355604.html},
  eprint = {https://fcihib.journals.ekb.eg/article_355604_5e1fffb346badcec53a709338b797cac.pdf},
  publisher={Helwan University, Faculty of Computers and Artificial Intelligence}
}

@INPROCEEDINGS{Taie2024Rehab,
  author={Taie, Alaa and Hamdy, Abdalla and Zaher, Moamen and Al-Emrany, Asmaa M. and Mahmoud Ahmed, Omnia Saeed and Atia, Ayman},
  booktitle={2024 14th International Conference on Electrical Engineering (ICEENG)}, 
  title={Physical Rehabilitation Exercises Classification Using Deep Learning Models}, 
  year={2024},
  month={5},
  volume={},
  number={},
  pages={181-183},
  keywords={Solid modeling;Accuracy;Three-dimensional displays;Surgery;Skeleton;Pattern recognition;Object recognition;Physical Rehabilitation;LSTM;CNN-LSTM;GRU},
  doi={10.1109/ICEENG58856.2024.10566467}
}
@INPROCEEDINGS{1$,
  author={Walid, Mazen and Ameen, Mostafa and Zaher, Moamen and Atia, Ayman},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Comparative Analysis of Movement Segmentation Techniques in Untrimmed Videos Using Optical Flow and Frame Differencing Using the $1 Unistroke Recognizer}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={147-152},
  keywords={Human computer interaction;Visualization;Adaptive systems;Motion segmentation;Surveillance;Machine learning;Streaming media;Untrimmed video analysis;Optical flow;Frame differencing;$1 Unistroke Recognizer;Movement segmentation;Cross-domain analysis},
  doi={10.1109/IMSA61967.2024.10652665},
  abstract={Untrimmed video analysis presents a complex challenge due to diverse movements across different domains, crucial for applications like action recognition, surveillance, and human-computer interaction. This study aims to compare the effectiveness of two main segmenting methods for untrimmed video segmentation and classification, addressing the challenge of diverse movements in various domains. Leveraging optical flow, frame differencing, and the 1$ algorithm, the performance of these methods in enabling robust and adaptable analysis of untrimmed video sequences is examined, facilitating accurate movement recognition across different contexts. Through a comparative study, the effectiveness and versatility of these segmenting methods are evaluated, revealing their capabilities to accurately segment and classify movements across different domains. The findings provide insights into the strengths and limitations of each method, offering implications for various fields, including computer vision, machine learning, and multimedia processing, and opening avenues for enhanced applications in diverse domains.}
}
@INPROCEEDINGS{rvoc,
  author={Hany, Steven and Samir, Mina and Zaher, Moamen},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Referring Video Object Clustering (RVOC)}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={336-341},
  keywords={Training;Surveillance;Clustering algorithms;Object detection;Solids;User experience;Object recognition},
  abstract={System surveillance involves the continuous monitoring and analysis of organizational data to ensure security and operational efficiency. This proactive approach aids in detecting anomalies, threats, and performance issues, thereby safeguarding sensitive information and maintaining system integrity. This paper proposes a framework for querying videos named Referring Video Object Clustering (RVOC). Traditional methods necessitate identifying the number of clusters before object identification based on pre-trained datasets, with components such as Principal Component Analysis (PCA) relying on fixed numerical values. In contrast, The proposed framework (RVOC) performs dynamic clustering without prior training, allowing the number of classes to adjust based on retrieved video objects. This method employs a sophisticated NLP query system, enabling intricate searches like “person skating on a red skateboard.” The system diligently searches and analyzes video content to find instances matching the query, grouping and segmenting detected individuals with advanced clustering algorithms. This enhances user experience by facilitating easy navigation and selection of desired individuals, with each cluster representing a unique person or object. RVOC's efficacy is evidenced by its Normalized Mutual Information (NMI) score of 51% in tests with multilabel car and color datasets.},
  doi={10.1109/IMSA61967.2024.10652781}
}
@INPROCEEDINGS{rehab_transformer,
  author={Hamdy, Abdalla and Taie, Alaa and Zaher, Moamen and Al-Emrany, Asmaa M. and Ahmed, Omnia Saeed Mahmoud and Atia, Ayman},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Enhancing Physical Therapy Through Transformer-Based Models: A Study on Exercise Classification}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={366-371},
  keywords={Deep learning;Accuracy;Surgery;Transformers;Cameras;Human activity recognition;Monitoring;Rehabilitation;HART;Vision transformers;LSTM;CNN-LSTM;CNN-GRU;GRU;BiLSTM},
  abstract={Physical therapy is crucial for recovering and enhancing the physical functions of patients. It also improves the recovery process after injuries, surgeries, or diseases. Home-based rehabilitation has become essential in healthcare, particularly due to the limitations of traditional rehabilitation methods. This study proposes a system for monitoring patients, tracking their progress throughout rehabilitation, and identifying the skeletal points for each exercise. Also, identifying exercises based on deep learning single models, transformer models, and fused models using datasets collected by an expert physiotherapist through RGB cameras. LSTM, CNN-GRU, CNN-LSTM, GRU, and BiLSTM were applied with accuracies of 93.33%, 91.96%, 92.86%, 93.33%, 91.11% respectively. DenseNet201 achieved a higher of 96.42% and ViT-CNN with 91.71%. Furthermore, the human activity recognition transformer (HART) achieved an accuracy score of 91.96%},
  doi={10.1109/IMSA61967.2024.10652817}
}
@INPROCEEDINGS{stress,
  author={Eldien, Noha A. Saad and Ali, Raghda Essam and Ezzeldin, Mustafa and Zaher, Moamen},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Unveiling Stress: A Comparative Analysis of Multimodal Sensor Fusion Techniques for Predictive Modeling}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={556-562},
  keywords={Temperature sensors;Heart rate;Temperature measurement;Temperature distribution;Multimodal sensors;Predictive models;Data models;Multi-modal;Fusion techniques;Stress Detection;Body Sensors},
  abstact={Stress in professional environments is a significant concern. Medical professionals are particularly vulnerable to stress, especially during emergencies. Nurses hold a vital position in delivering care within hospital settings. It's crucial to anticipate stress levels among nurses to help them perform their duties effectively and avoid the long-term impacts of stress. This study seeks to explore how body sensors and machine learning techniques can be employed to monitor physiological signs and identify stress levels among nurses. It utilizes a benchmark dataset collected from 15 different nurses, including signals such as heart rate (HR), Electrodermal Activity (EDA), and Skin Temperature alongside location data extracted by an accelerometer. This study explores several fusion strategies, such as data, model, and prediction fusion levels to improve the accuracy and reliability of stress prediction models. Through a comparative analysis, this paper highlights the strengths and limitations of diverse fusion techniques. shedding light on their efficacy in capturing the nuanced features of stress. The findings offer valuable insights into the optimization of multimodal sensor fusion for enhanced stress prediction, creating pathways for reliable frameworks in the healthcare domain. This research conducted a comparative study between 3 different levels: data-fusion, model-fusion, and prediction-fusion. Prediction-level fusion outperformed both model-level fusion by 1.97% and data-level fusion by 1.26%.},
  doi={10.1109/IMSA61967.2024.10652655}
}
@INPROCEEDINGS{driver,
  author={Diaaeldin, Ahmed and Zaher, Moamen},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Enhancing Road Safety: Leveraging CNN-LSTM and Bi-LSTM Models for Advanced Driver Behavior Detection}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={416-422},
  abstract={Recognizing the importance of driver behavior is essential for enhancing road safety and optimizing traffic management systems. This study employs advanced deep learning techniques, specifically CNN-LSTM and Bi-LSTM models, to refine the prediction of driver behaviors using sensor data from the Honda Research Institute Driving Dataset (HDD). Our approach integrates a robust dataset encompassing a broad spectrum of sensor inputs, from vehicle dynamics to driver operational parameters, propelling advancements in driver behavior detection. The methodologies utilized enable the discernment of subtle and complex driving patterns, contributing to the reduction of road safety hazards. Our findings indicate that these models significantly improve the detection of hazardous driving behaviors, surpassing previous state-of-the-art methodologies with notable gains in mean average precision (mAP). These advancements affirm the potential of deep learning technologies in crafting sophisticated predictive safety systems, paving the way for future innovations.},
  keywords={Deep learning;Technological innovation;Predictive models;Propulsion;Road safety;Data models;Vehicle dynamics;LSTM;CNN-LSTM;Bi-LSTM;Driver Behavior;Sensors},
  doi={10.1109/IMSA61967.2024.10652785}
}
@INPROCEEDINGS{brain_tumor,
  author={Ahmed, Mariam and Zaher, Moamen and Mohammed, Ammar},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Enhancing Brain Tumor Classification: A Comparative Study of Single-Model and Multi-Model Fusion Approaches}, 
  year={2024},
  volume={},
  number={},
  month={7},
  pages={495-500},
  keywords={Deep learning;Computer vision;Accuracy;Brain modeling;Transformers;Wavelet analysis;Discrete wavelet transforms;Multimodal;Model fusion;Image fusion;pre-trained;Deep Learning},
  abstract={Brain tumors are the leading cause of death world-wide. Deep learning has been successful in previous tasks like classification. However, it's being limited by the reliance on a single imaging modality which isn't enough, where a single modality can provide higher performance but is unreliable for accurate treatment and diagnosis. This study aims to improve brain tumor classification using deep learning and fusion techniques of multiple modalities. The study employs three fusion approaches: image-level fusion, feature-level fusion, and wavelet-based fusion. Extensive experiments were conducted on the BRATS2020 dataset. Initially, we train and evaluate the performance of 21 baseline models, encompassing 20 CNN-based architectures alongside the vision transformer model. Moreover, we identify the highest-performing models within each class for fusion. Furthermore, inspired by the baseline models, we dive deeper, introducing each modality as input to its respective best-performing model and fusing the outputs for multi-modality model-level fusion. Finally, we employ wavelet-based fusion to optimize information integration, implementing Discrete Wavelet Transform on our dataset. Model-level fusion outperformed image fusion across all evaluation metrics by 1 % accuracy, 4.7% precision, 6.6 % recall, and 0.7% F1-score.},
  doi={10.1109/IMSA61967.2024.10652770}
}
@INPROCEEDINGS{csrf,
  author={Ramadan, Mohamed and Osama, Bassem and Zaher, Moamen and Mansour, Hesham and El Sersi, Wael},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Enhancing Web Security: A Comparative Analysis of Machine Learning Models for CSRF Detection}, 
  year={2024},
  month={7},
  volume={},
  number={},
  pages={18-25},
  keywords={Analytical models;Machine learning algorithms;Forestry;Boosting;Forgery;Application security;Classification algorithms;CSRF;CatBoost;XGBoost;Extra Tree;Random Forest;LightGBM},
  abstract={This paper investigates the application of utilizing machine learning techniques to enhance Cross-Site Request Forgery (CSRF) detection in web applications. CSRF remains a critical security concern, consistently ranking among the top vulnerabilities in the Open Web Application Security Project (OWASP) list and Bugcrowd's ranking of global cybersecurity threats. We conduct a comparative analysis of sixteen machine learning algorithms, categorized as ensemble and non-ensemble methods. Our findings demonstrate that ensemble models, including Extreme Gradient Boosting and Extra Trees, achieve superior performance in identifying CSRF attacks compared to non-ensemble models. We evaluate the models using 5-fold and 10-fold cross-validation, consistently revealing the superiority of ensemble approaches. Notably, our proposed Extra Tree classifier surpasses the state-of-the-art Random Forest algorithm by 2.67% in recall and 1.16% in F1-score. These results highlight the potential of ensemble models for robust CSRF detection in web security.},
  doi={10.1109/IMSA61967.2024.10652629}
}


@article{zaher2024fusing,
title = {Fusing CNNs and attention-mechanisms to improve real-time indoor Human Activity Recognition for classifying home-based physical rehabilitation exercises},
  author={Zaher, Moamen and Ghoneim, Amr S and Abdelhamid, Laila and Atia, Ayman},
  journal={Computers in Biology and Medicine},
volume = {184},
pages = {109399},
year = {2025},
issn = {0010-4825},
  publisher={Elsevier},
  issn={0010-4825},
  eissn={1879-0534},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109399},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524014847},
  keywords = {Physical rehabilitation, Deep learning, Transfer learning, Vision Transformer (ViT), Model fusion, Continuous Wavelet Transform (CWT), Mel-Frequency Cepstral Coefficients (MFCC)},

}

@article{ZAHER2025109399,
title = {Fusing CNNs and attention-mechanisms to improve real-time indoor Human Activity Recognition for classifying home-based physical rehabilitation exercises},
journal = {Computers in Biology and Medicine},
volume = {184},
pages = {109399},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109399},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524014847},
author = {Moamen Zaher and Amr S. Ghoneim and Laila Abdelhamid and Ayman Atia},
keywords = {Physical rehabilitation, Deep learning, Transfer learning, Vision Transformer (ViT), Model fusion, Continuous Wavelet Transform (CWT), Mel-Frequency Cepstral Coefficients (MFCC)},
abstract = {Physical rehabilitation plays a critical role in enhancing health outcomes globally. However, the shortage of physiotherapists, particularly in developing countries where the ratio is approximately ten physiotherapists per million people, poses a significant challenge to effective rehabilitation services. The existing literature on rehabilitation often falls short in data representation and the employment of diverse modalities, limiting the potential for advanced therapeutic interventions. To address this gap, This study integrates Computer Vision and Human Activity Recognition (HAR) technologies to support home-based rehabilitation. The study mitigates this gap by exploring various modalities and proposing a framework for data representation. We introduce a novel framework that leverages both Continuous Wavelet Transform (CWT) and Mel-Frequency Cepstral Coefficients (MFCC) for skeletal data representation. CWT is particularly valuable for capturing the time-frequency characteristics of dynamic movements involved in rehabilitation exercises, enabling a comprehensive depiction of both temporal and spectral features. This dual capability is crucial for accurately modelling the complex and variable nature of rehabilitation exercises. In our analysis, we evaluate 20 CNN-based models and one Vision Transformer (ViT) model. Additionally, we propose 12 hybrid architectures that combine CNN-based models with ViT in bi-model and tri-model configurations. These models are rigorously tested on the UI-PRMD and KIMORE benchmark datasets using key evaluation metrics, including accuracy, precision, recall, and F1-score, with 5-fold cross-validation. Our evaluation also considers real-time performance, model size, and efficiency on low-power devices, emphasising practical applicability. The proposed fused tri-model architectures outperform both single-architectures and bi-model configurations, demonstrating robust performance across both datasets and making the fused models the preferred choice for rehabilitation tasks. Our proposed hybrid model, DenMobVit, consistently surpasses state-of-the-art methods, achieving accuracy improvements of 2.9% and 1.97% on the UI-PRMD and KIMORE datasets, respectively. These findings highlight the effectiveness of our approach in advancing rehabilitation technologies and bridging the gap in physiotherapy services.}
}